---
title: "p8105_hw5_bf2506"
author: "bf2506"
date: "2022-11-15"
output: github_document
---
```{r load package, message=FALSE, warning=FALSE}
library(tidyverse)

```

### Problem 2

```{r read homi_data}
homi_data = 
  read.csv("./data/homicide-data.csv") %>%
  janitor::clean_names()
```
**Describe the raw data:**

* The dataset has `r ncol(homi_data)` variables and `r nrow(homi_data)` observations. (Variables are as follows)

  * "uid"(the abbreviation of city - number); "reported_date"
  
  * "victim_last"(last name of victim); "victim_first"(first name of victim); "victim_race"; "victim_age"; "victim_sex"
  
  * "city"; "state"; "lat"(latitude); "lon"(longitude); 
  
  * "disposition": 
    * Closed by arrest when *police reported that to be the case*
    * Closed without arrest when *police reported that to be “exceptionally cleared.”* Those are cases in which there is sufficient evidence but an arrest is not possible, for example, if the suspect has died.
    * Open/No arrest: *All other cases* were classified as having no arrest.

##### Part A

Create a city_state variable and then summarize within cities to obtain the total number of homicides and the number of unsolved homicides.

```{r message=FALSE, warning=FALSE}
homi_data = 
  homi_data %>% 
  mutate(city_state = paste(city, state, sep = ", ", collapse = NULL))

total_df = 
  homi_data %>% 
  group_by(city_state) %>% 
  summarize(total_number_homicide = n())

unsolved_df = 
  homi_data %>%
  filter(disposition != "Closed by arrest") %>% 
  group_by(city_state) %>% 
  summarize(number_unsolved_homicide = n())

total_unsolved_df = 
  left_join(total_df, unsolved_df) %>% 
  replace(is.na(.), 0)
```
*Tulsa, AL doesn't have a unsolved homicide, so it is not included in unsolved_df.*

##### Part B

For the city of Baltimore, MD, use the prop.test function to estimate the proportion of homicides that are unsolved; save the output of prop.test as an R object, apply the broom::tidy to this object and pull the estimated proportion and confidence intervals from the resulting tidy dataframe.

```{r message=FALSE, warning=FALSE}
balt_prop = 
  prop.test(1825, 2827, p = NULL, 
            alternative = "two.sided", conf.level = 0.95, correct = TRUE)
balt_prop

balt_prop_tibble = broom::tidy(balt_prop)
balt_prop_tibble

balt_prop_tibble %>% pull(estimate)
balt_prop_tibble %>% pull(conf.low)
balt_prop_tibble %>% pull(conf.high)
balt_prop_tibble %>% mutate(ci = paste("(", round(conf.low, 4), ",", round(conf.high, 4), ")")) %>% pull(ci)
```

##### Part C

Run prop.test for every city. Extract *the proportion of unsolved homicides* and *the confidence interval*. Do this within a “tidy” pipeline, making use of purrr::map, purrr::map2, list columns and unnest as necessary to create a tidy dataframe with estimated proportions and CIs for each city.

```{r message=FALSE, warning=FALSE}
prop_df = 
  total_unsolved_df %>% 
  filter(city_state != "Tulsa, AL") %>% 
  mutate(result = purrr::map2(.x = number_unsolved_homicide, .y = total_number_homicide, ~broom::tidy(prop.test(x = .x, n = .y, alternative = "two.sided", conf.level = 0.95, correct = TRUE)))) %>% 
  unnest(result) %>% 
  select(city_state, estimate, conf.low, conf.high) %>% 
  mutate(confidence_interval = paste("(", round(conf.low, 4), ",", round(conf.high, 4), ")"))

prop_df
```
Considering the total number of homicides is 1 and the number of unsolved homicides is 0 in Tulsa, AL, the data is too small and insufficient in prop.test. I drop Tulsa, AL data.

##### Part C

Create a plot that shows the estimates and CIs for each city – check out geom_errorbar for a way to add error bars based on the upper and lower limits. Organize cities according to the proportion of unsolved homicides

```{r}
Estimates_CIs_Plot = 
  prop_df %>% 
  ggplot(aes(x = reorder(city_state, estimate), y = estimate)) + 
  geom_point() +
  geom_errorbar(aes(ymin = conf.low, ymax = conf.high)) +
  theme(axis.text.x = element_text(size = 9, face = "bold", angle = 70, hjust = 1), 
        plot.title = element_text(hjust = 0.5)) +
  labs(title = "Estimate And CI Of Proportion Of Unsolved Homicides For Each City", x = "City, State", y = "Estimate / CI")

Estimates_CIs_Plot
```

Since I discussed aboved, the  total number of homicides and the number of unsolved homicides in Tulsa, AL are not sufficient and enough to analyze and run a prop.test, the "Estimate And CI Of Proportion Of Unsolved Homicides For Each City" plot doesn't include Tulsa, AL data.

### Problem 3

```{r}
set.seed(1)
```

##### Part A 

Generate 5000 datasets from the model(x ~ normal(mu, sigma)). For each dataset, save mu_cap and the p-value arising from t.test(significance level = 0,05)
```{r}
output = vector("list", 5000)

for (i in 1:5000) {
  output[[i]] = 
    broom::tidy(t.test( x = rnorm(n = 30, mean = 0, sd = 5), alternative = "two.sided", mu = 0, paired = FALSE, conf.level = 0.95)) %>% 
    select(estimate, p.value) %>% 
    rename(mu_cap = estimate)
}

sim_results_0 = bind_rows(output)
sim_results_0
```

##### Part B

```{r}
sim_mu = function(mu) {
  output = vector("list", 5000)
  
  for (i in 1:5000) {
  output[[i]] = 
    broom::tidy(t.test( x = rnorm(n = 30, mean = mu, sd = 5), alternative = "two.sided", mu = 0, paired = FALSE, conf.level = 0.95)) %>% 
    select(estimate, p.value) %>% 
    rename(mu_cap = estimate) }

sim_results_mu = bind_rows(output)
}
```

Repeat for mu=1,2,3,4,5,6
```{r}
sim_df = 
  expand_grid(
    mu = 1:6,
    iter = 1
  ) %>% 
  mutate(
    estimate_df = map(mu, sim_mu)
  ) %>% 
  unnest(estimate_df)
```

##### Part C
Make a plot showing the proportion of times the null was rejected (the power of the test) on the y axis and the true value of mu on the x axis. Describe the association between effect size and power.
```{r message=FALSE, warning=FALSE}
Power_Truemu_Plot = 
  sim_df %>% 
  filter(p.value < 0.05) %>% 
  group_by(mu) %>% 
  summarize(times = n()) %>% 
  ggplot(aes(x = as.factor(mu), y = times/5000)) + 
  geom_point(alpha = .6) +
  theme(plot.title = element_text(hjust = 0.5)) +
  labs(title = "The Power Of The Test Of Every True MU", x = "True Value Of Mu", y = "Proportion Of Times Null Was Rejected ")

Power_Truemu_Plot
```
* Describe the association between effect size and power.
The effect size is the difference between the true mu and the mu we assumed (mu = 0) in this situation), so when the true mu goes from 1 to 6, the effect size increases, then we found the power of the test also increases. We can conclude that when everything else doesn't change, the power of test will increase if the effect size increases.

##### Part D

Make a plot showing the average estimate of mu_cap on the y axis and the true value of mu on the x axis. 
```{r}
Ave_True_plot = 
  sim_df %>% 
  group_by(mu) %>% 
  summarize(ave_mu_cap = mean(mu_cap)) %>% 
  ggplot(aes(x = as.factor(mu), y = ave_mu_cap)) + 
  geom_point(alpha = .6) +
  theme(plot.title = element_text(hjust = 0.5)) +
  labs(title = "Average Estimate Of Mu_cap Of Every True Mu", x = "True Value Of Mu", y = "Average Estimate Of Mu_cap")

Ave_True_plot
```

Make a second plot the average estimate of mu_cap only in samples for which the null was rejected on the y axis and the true value of mu on the x axis. 
```{r}
Ave_In_Reject_Plot = 
  sim_df %>% 
  filter(p.value < 0.05) %>% 
  group_by(mu) %>% 
  summarize(ave_reject = mean(mu_cap)) %>% 
  ggplot(aes(x = as.factor(mu), y = ave_reject)) + 
  geom_point(alpha = .6) +
  theme(plot.title = element_text(hjust = 0.5)) +
  labs(title = "Average Estimate Of Mu_cap Of Every True Mu In Reject-Null-Sample", x = "True Value Of Mu", y = "Average Estimate Of Mu_cap In Reject-Null-Sample")

Ave_In_Reject_Plot
```

* The sample average of mu_cap across tests for which the null is rejected is approximately equal to the true value of mu. When the true value of mu increases(above 4 in this plot), the sample average of mu_cap across tests for which the null is rejected will be more close to/equal to the true value of mu. 
* This is because when the true value of mu increases, the effect size increases, then the power of the test will also increase. Since the power of test increases, the probability of reject the false null hypothesis will increase, then the estimate mu (mu_cap) that is not proper/suitable is been rejected, so the sample average of mu_cap will approximately equal to the true value of mu.